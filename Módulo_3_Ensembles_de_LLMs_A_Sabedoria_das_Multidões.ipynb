{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Assaoka/Guide-to-Advanced-LLM-Techniques/blob/main/M%C3%B3dulo_3_Ensembles_de_LLMs_A_Sabedoria_das_Multid%C3%B5es.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Módulo 3: Ensembles de LLMs - A Sabedoria das Multidões"
      ],
      "metadata": {
        "id": "wg-qHhguTI8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução"
      ],
      "metadata": {
        "id": "nCz1fxpITKUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nos módulos anteriores, focamos em como aprimorar a interação com um único modelo através da engenharia de prompt. Agora, vamos explorar uma técnica poderosa emprestada do aprendizado de máquina tradicional: os ensembles.\n",
        "\n",
        "A ideia central de um ensemble é que \"várias cabeças pensam melhor que uma\". Em vez de confiar na resposta de um único modelo, combinamos as predições de múltiplos \"especialistas\" para chegar a uma decisão final mais robusta e precisa. LLMs, apesar de seu poder, podem ser suscetíveis a vieses, erros factuais ou \"alucinações\". Usar um ensemble pode mitigar esses riscos e aumentar a confiabilidade geral do sistema.\n",
        "\n",
        "Neste módulo, vamos explorar duas estratégias de ensemble, conforme detalhado no artigo:\n",
        "1. **Votação Majoritária:** A forma mais simples de ensemble, onde consultamos vários especialistas e adotamos a resposta da maioria.\n",
        "2. **Negociação:** Uma abordagem mais avançada que simula um debate estruturado entre modelos para refinar ideias e chegar a um consenso."
      ],
      "metadata": {
        "id": "X8eAMCj1TNBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Votação Majoritária"
      ],
      "metadata": {
        "id": "uiqIvXByTQ1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A votação é a forma mais intuitiva de ensemble. A ideia é consultar vários \"especialistas\" independentes e adotar a resposta da maioria. No contexto de LLMs, esses especialistas podem ser:\n",
        "1. Diferentes Modelos: Você pode fazer a mesma pergunta para o Llama 3, Mixtral e Sabiá, e depois contar os votos de cada um.\n",
        "2. Diferentes Prompts: Usar vários prompts (ex: um Zero-Shot, um Few-Shot) com o mesmo modelo.\n",
        "3. Múltiplas Execuções do Mesmo Modelo: Esta é a abordagem conhecida como Self-Consistency (Autoconsistência), proposta por Wang et al. (2022) [1]. Executamos o mesmo prompt várias vezes com uma temperatura > 0 para gerar respostas diversas e escolhemos a mais frequente.\n",
        "\n",
        "Vamos implementar a abordagem de Self-Consistency, pois é a mais prática quando se tem acesso a uma única API."
      ],
      "metadata": {
        "id": "Dw-_YBJaTVDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-core langchain-community langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPiVCK-qTkGr",
        "outputId": "f1878ed1-2fce-4bf0-dd29-3e861f4ad812"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "\n",
        "llm_groq = ChatGroq(\n",
        "    model=\"gemma2-9b-it\",\n",
        "    api_key=userdata.get('GROQ_API_KEY'),\n",
        "    temperature=0.7\n",
        ")"
      ],
      "metadata": {
        "id": "uChGcceXTsJX",
        "outputId": "9e54b90e-c73d-4c4b-917b-27d51fe07099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret GROQ_API_KEY does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3858537229.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m llm_groq = ChatGroq(\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gemma2-9b-it\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GROQ_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret GROQ_API_KEY does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "class ClassificacaoNoticia(BaseModel):\n",
        "    emocao: Literal[\"Felicidade\", \"Tristeza\", \"Nojo\", \"Raiva\",\n",
        "                    \"Medo\", \"Surpreza\", \"Desprezo\"] = Field(\n",
        "                    description=\"A emoção primária transmitida pelo texto. Estamos usando como padrão as emoções universais de ekman\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=ClassificacaoNoticia)\n",
        "formato = parser.get_format_instructions()\n"
      ],
      "metadata": {
        "id": "OFNQQIZOTyKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Classifique a notícia abaixo quanto a emoção.\n",
        "{format_instructions}\n",
        "Notícia: {noticia}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    template=template,\n",
        "    partial_variables={\"format_instructions\": formato}\n",
        ")"
      ],
      "metadata": {
        "id": "aFjuHftWUmnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm_groq | parser\n",
        "chain.invoke(\"Nesse cenário de instabilidade, os investidores não sabem o que fazer.\")"
      ],
      "metadata": {
        "id": "OX7wfKtEU_SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def votacao(chain, texto: str, n: int) -> ClassificacaoNoticia:\n",
        "    votos = [chain.invoke(texto) for _ in range(n)]\n",
        "    vals = [voto.emocao for voto in votos]\n",
        "    conts = {}\n",
        "    for val in vals:\n",
        "        print(val)\n",
        "        if val in conts:\n",
        "            conts[val] += 1\n",
        "        else:\n",
        "            conts[val] = 1\n",
        "\n",
        "    return ClassificacaoNoticia(emocao=max(conts, key=conts.get))\n",
        "\n",
        "\n",
        "votacao(chain, \"Os resultados fiscais do primeiro semestre de 2024 indicam que, no curto prazo, a meta fiscal de -0,25% do PIB é mais viável, apesar das incertezas de médio e longo prazo devido aos desafios fiscais e ao aumento das despesas obrigatórias. O desempenho das receitas líquidas foi positivo, impulsionado pelas medidas legislativas de arrecadação, mas o crescimento das despesas, especialmente com benefícios previdenciários, elevou o déficit primário para R$ 68,7 bilhões, agravando o cenário fiscal. Apesar das medidas de contenção adotadas, a sustentabilidade das contas públicas a longo prazo permanece ameaçada, exigindo soluções concretas e uma postura cautelosa para os próximos anos.\", 3)"
      ],
      "metadata": {
        "id": "LVA9DfWvUfOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Negociação"
      ],
      "metadata": {
        "id": "pAWUfO0TWrYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "E se, em vez de votar de forma independente, os \"especialistas\" pudessem debater e refinar suas ideias? Essa é a premissa da Negociação, uma técnica que simula um processo de argumentação para resolver discordâncias. O trabalho de Sun et al. (2023) [2] explora essa ideia, mostrando que um debate estruturado pode ajudar a resolver ambiguidades.\n",
        "\n",
        "Vamos implementar um framework de debate iterativo com dois papéis:\n",
        "1. Gerador: Gera a análise e classificação inicial.\n",
        "2. Discriminador: Revisa a análise do propositor, busca falhas, pontos de vista alternativos ou ambiguidades e oferece uma contra-análise.\n",
        "\n",
        "O processo é um loop:\n",
        "1. O Gerador faz uma análise inicial.\n",
        "2. O Discriminador avalia a análise. Podendo concordar ou apresentar uma contra proposta.\n",
        "3. O Propositor revisa sua análise com base na crítica. E pode concordar ou enviar uma contra proposta.\n",
        "4. O ciclo se repete até um número máximo de rodadas ou até que um consenso seja alcançado."
      ],
      "metadata": {
        "id": "I-9IIHbhWwAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gerador = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    api_key=userdata.get('GROQ_API_KEY'),\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "discriminador = ChatGroq(\n",
        "    model=\"qwen-qwq-32b\",\n",
        "    api_key=userdata.get('GROQ_API_KEY'),\n",
        "    temperature=0.7\n",
        ")"
      ],
      "metadata": {
        "id": "3aIjooGSXfJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Negociacao(BaseModel):\n",
        "    raciocinio: str = Field(description=\"Linha de raciocínio passo a passo do modelo até chegar em uma decisão.\")\n",
        "    concordo: bool = Field(description=\"Representa se o modelo concorda ou não com o modelo anterior. Inicia como False.\")\n",
        "    emocao: Literal[\"Felicidade\", \"Tristeza\", \"Nojo\", \"Raiva\",\n",
        "                    \"Medo\", \"Surpreza\", \"Desprezo\"] = Field(\n",
        "                    description=\"A emoção primária transmitida pelo texto. Estamos usando como padrão as emoções universais de ekman\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Negociacao)\n",
        "formato = parser.get_format_instructions()"
      ],
      "metadata": {
        "id": "JwCg4cKgX_uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gerador1_template = \"\"\"Classifique a notícia abaixo quanto a emoção.\n",
        "{format_instructions}\n",
        "Notícia: {noticia}\n",
        "\"\"\"\n",
        "\n",
        "gerador1_prompt = PromptTemplate.from_template(\n",
        "    template=gerador1_template,\n",
        "    partial_variables={\"format_instructions\": formato}\n",
        ")"
      ],
      "metadata": {
        "id": "BU3IYrTSX7NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminador1_template = \"\"\"Você está em um debate com outro agente. A tarefa de vocês é classificar a notícia abaixo quanto a emoção.\n",
        "{format_instructions}\n",
        "Notícia: {noticia}\n",
        "Gerador: {gerador}\n",
        "\"\"\"\n",
        "\n",
        "discriminador1_prompt = PromptTemplate.from_template(\n",
        "    template=discriminador1_template,\n",
        "    partial_variables={\"format_instructions\": formato}\n",
        ")"
      ],
      "metadata": {
        "id": "TID39dN4YzFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterações_template = \"\"\"Você está em um debate com outro agente. A tarefa de vocês é classificar a notícia abaixo quanto a emoção.\n",
        "{format_instructions}\n",
        "Notícia: {noticia}\n",
        "sua proposta: {analise}\n",
        "Contra proposta: {contra_proposta}\n",
        "\"\"\"\n",
        "\n",
        "iterações_prompt = PromptTemplate.from_template(\n",
        "    template=iterações_template,\n",
        "    partial_variables={\"format_instructions\": formato}\n",
        ")"
      ],
      "metadata": {
        "id": "YgyY6_weZAQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inicio = gerador1_prompt | gerador | parser\n",
        "discriminar = discriminador1_prompt | discriminador | parser\n",
        "iterações1 = iterações_prompt | gerador | parser\n",
        "iterações2 = iterações_prompt | discriminador | parser"
      ],
      "metadata": {
        "id": "pafHW1-LZjAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def negociacao(noticia: str, n: int = 3):\n",
        "    historico = [inicio.invoke({\"noticia\": noticia})]\n",
        "    historico.append(discriminar.invoke({\"noticia\": noticia, \"gerador\": historico[-1].raciocinio}))\n",
        "    for i in range(n-2):\n",
        "        if i % 2 == 0:\n",
        "            historico.append(iterações1.invoke({\"noticia\": noticia,\n",
        "                                                \"analise\": historico[-2].raciocinio,\n",
        "                                                \"contra_proposta\": historico[-1].raciocinio}))\n",
        "        else:\n",
        "            historico.append(iterações2.invoke({\"noticia\": noticia,\n",
        "                                                \"analise\": historico[-2].raciocinio,\n",
        "                                                \"contra_proposta\": historico[-1].raciocinio}))\n",
        "        if historico[-1].concordo:\n",
        "            break\n",
        "    return historico\n"
      ],
      "metadata": {
        "id": "XGTkIrx1ZUlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negociacao(\"Os resultados fiscais do primeiro semestre de 2024 indicam que, no curto prazo, a meta fiscal de -0,25% do PIB é mais viável, apesar das incertezas de médio e longo prazo devido aos desafios\")"
      ],
      "metadata": {
        "id": "rxXstJYwaNw7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}